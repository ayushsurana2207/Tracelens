// Mock data for all pages
export const mockTraces = [
  {
    id: 1,
    model: "gpt-4",
    provider: "openai",
    latency_ms: 1250,
    tokens: 450,
    cost_usd: 0.0067,
    status: "success" as const,
    created_at: new Date(Date.now() - 1000 * 60 * 5).toISOString(),
    input_tokens: 200,
    output_tokens: 250,
    prompt_tokens: 200,
    completion_tokens: 250,
    error_message: undefined,
    request_id: "req_123456",
    user_id: "user_1",
    endpoint: "/v1/chat/completions",
    temperature: 0.7,
    max_tokens: 1000,
    metadata: { conversation_id: "conv_123", session_id: "session_456" }
  },
  {
    id: 2,
    model: "claude-3-sonnet",
    provider: "anthropic",
    latency_ms: 980,
    tokens: 320,
    cost_usd: 0.0048,
    status: "success" as const,
    created_at: new Date(Date.now() - 1000 * 60 * 12).toISOString(),
    input_tokens: 150,
    output_tokens: 170,
    prompt_tokens: 150,
    completion_tokens: 170,
    error_message: undefined,
    request_id: "req_123457",
    user_id: "user_2",
    endpoint: "/v1/messages",
    temperature: 0.5,
    max_tokens: 800,
    metadata: { conversation_id: "conv_124", session_id: "session_457" }
  },
  {
    id: 3,
    model: "gemini-pro",
    provider: "google",
    latency_ms: 2100,
    tokens: 680,
    cost_usd: 0.0034,
    status: "failure" as const,
    created_at: new Date(Date.now() - 1000 * 60 * 18).toISOString(),
    input_tokens: 300,
    output_tokens: 380,
    prompt_tokens: 300,
    completion_tokens: 380,
    error_message: "API rate limit exceeded",
    request_id: "req_123458",
    user_id: "user_3",
    endpoint: "/v1beta/models/gemini-pro:generateContent",
    temperature: 0.8,
    max_tokens: 1500,
    metadata: { conversation_id: "conv_125", session_id: "session_458" }
  },
  {
    id: 4,
    model: "gpt-4-turbo",
    provider: "openai",
    latency_ms: 750,
    tokens: 280,
    cost_usd: 0.0084,
    status: "success" as const,
    created_at: new Date(Date.now() - 1000 * 60 * 25).toISOString(),
    input_tokens: 120,
    output_tokens: 160,
    prompt_tokens: 120,
    completion_tokens: 160,
    error_message: undefined,
    request_id: "req_123459",
    user_id: "user_1",
    endpoint: "/v1/chat/completions",
    temperature: 0.3,
    max_tokens: 500,
    metadata: { conversation_id: "conv_126", session_id: "session_459" }
  },
  {
    id: 5,
    model: "claude-3-haiku",
    provider: "anthropic",
    latency_ms: 450,
    tokens: 180,
    cost_usd: 0.0018,
    status: "success" as const,
    created_at: new Date(Date.now() - 1000 * 60 * 32).toISOString(),
    input_tokens: 80,
    output_tokens: 100,
    prompt_tokens: 80,
    completion_tokens: 100,
    error_message: undefined,
    request_id: "req_123460",
    user_id: "user_4",
    endpoint: "/v1/messages",
    temperature: 0.2,
    max_tokens: 300,
    metadata: { conversation_id: "conv_127", session_id: "session_460" }
  }
];

export const mockMetricsSummary = {
  total_requests: 1247,
  avg_latency_ms: 1150.5,
  total_tokens: 456789,
  total_cost_usd: 12.45,
  success_rate_pct: 94.2,
  failure_rate_pct: 5.8,
  p95_latency_ms: 2800.0,
  p99_latency_ms: 4500.0,
  total_input_tokens: 234567,
  total_output_tokens: 222222,
  avg_tokens_per_request: 366.5,
  cost_per_token: 0.000027,
  requests_per_minute: 8.7,
  error_rate_pct: 5.8
};

export const mockModelsSummary = [
  {
    model: "gpt-4",
    provider: "openai",
    total_requests: 456,
    avg_latency_ms: 1250.0,
    total_tokens: 156789,
    total_cost_usd: 4.67,
    success_count: 432,
    failure_count: 24,
    success_rate_pct: 94.7
  },
  {
    model: "claude-3-sonnet",
    provider: "anthropic",
    total_requests: 234,
    avg_latency_ms: 980.0,
    total_tokens: 78901,
    total_cost_usd: 2.34,
    success_count: 228,
    failure_count: 6,
    success_rate_pct: 97.4
  },
  {
    model: "gemini-pro",
    provider: "google",
    total_requests: 189,
    avg_latency_ms: 2100.0,
    total_tokens: 67890,
    total_cost_usd: 1.89,
    success_count: 165,
    failure_count: 24,
    success_rate_pct: 87.3
  },
  {
    model: "gpt-4-turbo",
    provider: "openai",
    total_requests: 298,
    avg_latency_ms: 750.0,
    total_tokens: 89123,
    total_cost_usd: 2.67,
    success_count: 285,
    failure_count: 13,
    success_rate_pct: 95.6
  },
  {
    model: "claude-3-haiku",
    provider: "anthropic",
    total_requests: 70,
    avg_latency_ms: 450.0,
    total_tokens: 24086,
    total_cost_usd: 0.88,
    success_count: 68,
    failure_count: 2,
    success_rate_pct: 97.1
  }
];

export const mockTimeSeriesData = {
  latency_ms: [
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(), value: 1200 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 23).toISOString(), value: 1350 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 22).toISOString(), value: 1100 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 21).toISOString(), value: 1450 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 20).toISOString(), value: 1300 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 19).toISOString(), value: 1250 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 18).toISOString(), value: 1400 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 17).toISOString(), value: 1150 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 16).toISOString(), value: 1300 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 15).toISOString(), value: 1200 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 14).toISOString(), value: 1350 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 13).toISOString(), value: 1100 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 12).toISOString(), value: 1250 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 11).toISOString(), value: 1400 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 10).toISOString(), value: 1150 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 9).toISOString(), value: 1300 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 8).toISOString(), value: 1200 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 7).toISOString(), value: 1350 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 6).toISOString(), value: 1100 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 5).toISOString(), value: 1250 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 4).toISOString(), value: 1400 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 3).toISOString(), value: 1150 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 2).toISOString(), value: 1300 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 1).toISOString(), value: 1200 },
    { timestamp: new Date().toISOString(), value: 1250 }
  ],
  cost_usd: [
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(), value: 0.45 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 23).toISOString(), value: 0.52 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 22).toISOString(), value: 0.38 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 21).toISOString(), value: 0.61 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 20).toISOString(), value: 0.48 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 19).toISOString(), value: 0.55 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 18).toISOString(), value: 0.42 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 17).toISOString(), value: 0.49 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 16).toISOString(), value: 0.56 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 15).toISOString(), value: 0.43 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 14).toISOString(), value: 0.50 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 13).toISOString(), value: 0.37 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 12).toISOString(), value: 0.44 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 11).toISOString(), value: 0.51 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 10).toISOString(), value: 0.38 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 9).toISOString(), value: 0.45 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 8).toISOString(), value: 0.52 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 7).toISOString(), value: 0.39 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 6).toISOString(), value: 0.46 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 5).toISOString(), value: 0.53 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 4).toISOString(), value: 0.40 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 3).toISOString(), value: 0.47 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 2).toISOString(), value: 0.54 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 1).toISOString(), value: 0.41 },
    { timestamp: new Date().toISOString(), value: 0.48 }
  ],
  tokens: [
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(), value: 1250 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 23).toISOString(), value: 1420 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 22).toISOString(), value: 1080 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 21).toISOString(), value: 1560 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 20).toISOString(), value: 1320 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 19).toISOString(), value: 1480 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 18).toISOString(), value: 1140 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 17).toISOString(), value: 1300 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 16).toISOString(), value: 1460 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 15).toISOString(), value: 1120 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 14).toISOString(), value: 1280 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 13).toISOString(), value: 1440 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 12).toISOString(), value: 1100 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 11).toISOString(), value: 1260 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 10).toISOString(), value: 1420 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 9).toISOString(), value: 1080 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 8).toISOString(), value: 1240 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 7).toISOString(), value: 1400 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 6).toISOString(), value: 1060 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 5).toISOString(), value: 1220 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 4).toISOString(), value: 1380 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 3).toISOString(), value: 1040 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 2).toISOString(), value: 1200 },
    { timestamp: new Date(Date.now() - 1000 * 60 * 60 * 1).toISOString(), value: 1360 },
    { timestamp: new Date().toISOString(), value: 1020 }
  ]
};

export const mockAgentSessions = [
  {
    id: 1,
    started_at: new Date(Date.now() - 1000 * 60 * 30).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 5).toISOString(),
    user_id: "user_1",
    title: "Customer Support Bot Session",
    status: "completed",
    total_latency_ms: 8500,
    total_tokens: 1200,
    total_cost_usd: 0.024,
    error_message: undefined,
    metadata: { environment: "production", version: "v2.1", region: "us-east" }
  },
  {
    id: 2,
    started_at: new Date(Date.now() - 1000 * 60 * 45).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 20).toISOString(),
    user_id: "user_2",
    title: "Code Review Assistant",
    status: "completed",
    total_latency_ms: 12000,
    total_tokens: 2100,
    total_cost_usd: 0.042,
    error_message: undefined,
    metadata: { environment: "staging", version: "v2.0", region: "us-west" }
  },
  {
    id: 3,
    started_at: new Date(Date.now() - 1000 * 60 * 15).toISOString(),
    ended_at: undefined,
    user_id: "user_3",
    title: "Document Analysis Pipeline",
    status: "running",
    total_latency_ms: 0,
    total_tokens: 0,
    total_cost_usd: 0,
    error_message: undefined,
    metadata: { environment: "development", version: "v1.9", region: "eu-west" }
  },
  {
    id: 4,
    started_at: new Date(Date.now() - 1000 * 60 * 60).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 35).toISOString(),
    user_id: "user_4",
    title: "Multi-Agent Research Task",
    status: "failed",
    total_latency_ms: 15000,
    total_tokens: 1800,
    total_cost_usd: 0.036,
    error_message: "Agent timeout after 25 minutes",
    metadata: { environment: "production", version: "v2.1", region: "asia-pacific" }
  },
  {
    id: 5,
    started_at: new Date(Date.now() - 1000 * 60 * 90).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 70).toISOString(),
    user_id: "user_1",
    title: "Content Generation Workflow",
    status: "completed",
    total_latency_ms: 6500,
    total_tokens: 950,
    total_cost_usd: 0.019,
    error_message: undefined,
    metadata: { environment: "production", version: "v2.0", region: "us-east" }
  }
];

export const mockAgentSpans = [
  {
    id: 1,
    session_id: 1,
    parent_id: undefined,
    span_type: "agent" as const,
    name: "MainAgent",
    status: "success" as const,
    latency_ms: 5000,
    prompt: "You are a helpful customer support assistant. Please help the user with their request.",
    output: "I'll help you with that request. Let me process this step by step.",
    error: undefined,
    created_at: new Date(Date.now() - 1000 * 60 * 30).toISOString(),
    started_at: new Date(Date.now() - 1000 * 60 * 30).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 25).toISOString(),
    tokens_used: 300,
    cost_usd: 0.006,
    model_used: "gpt-4",
    provider_used: "openai",
    tool_calls: undefined,
    reasoning_steps: undefined,
    metadata: { priority: "high", retry_count: 0 },
    trace_id: "trace_12345"
  },
  {
    id: 2,
    session_id: 1,
    parent_id: 1,
    span_type: "tool" as const,
    name: "Tool_1",
    status: "success" as const,
    latency_ms: 1500,
    prompt: undefined,
    output: "Tool executed successfully",
    error: undefined,
    created_at: new Date(Date.now() - 1000 * 60 * 25).toISOString(),
    started_at: new Date(Date.now() - 1000 * 60 * 25).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 23).toISOString(),
    tokens_used: 50,
    cost_usd: 0.001,
    model_used: undefined,
    provider_used: undefined,
    tool_calls: {
      tool_name: "search_knowledge_base",
      parameters: { query: "customer issue", limit: 10 },
      result: "Found 5 relevant articles"
    },
    reasoning_steps: undefined,
    metadata: { iteration: 1, priority: "medium", retry_count: 0 },
    trace_id: "trace_12346"
  },
  {
    id: 3,
    session_id: 1,
    parent_id: 1,
    span_type: "reasoning" as const,
    name: "Reasoning_1",
    status: "success" as const,
    latency_ms: 2000,
    prompt: "Analyze the customer's issue and determine the best solution approach",
    output: "Based on the issue description, I recommend checking the account settings first",
    error: undefined,
    created_at: new Date(Date.now() - 1000 * 60 * 23).toISOString(),
    started_at: new Date(Date.now() - 1000 * 60 * 23).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 21).toISOString(),
    tokens_used: 200,
    cost_usd: 0.004,
    model_used: "gpt-4",
    provider_used: "openai",
    tool_calls: undefined,
    reasoning_steps: [
      "Step 1: Analyze customer issue description",
      "Step 2: Identify potential root causes",
      "Step 3: Determine solution approach",
      "Step 4: Validate solution feasibility"
    ],
    metadata: { iteration: 1, priority: "high", retry_count: 0 },
    trace_id: "trace_12347"
  },
  {
    id: 4,
    session_id: 1,
    parent_id: 1,
    span_type: "llm_call" as const,
    name: "LLM_Call_1",
    status: "success" as const,
    latency_ms: 1000,
    prompt: "Generate a helpful response to the customer's issue",
    output: "Thank you for contacting us. I've identified the issue and here's how to resolve it...",
    error: undefined,
    created_at: new Date(Date.now() - 1000 * 60 * 21).toISOString(),
    started_at: new Date(Date.now() - 1000 * 60 * 21).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 20).toISOString(),
    tokens_used: 150,
    cost_usd: 0.003,
    model_used: "gpt-4",
    provider_used: "openai",
    tool_calls: undefined,
    reasoning_steps: undefined,
    metadata: { iteration: 1, priority: "high", retry_count: 0 },
    trace_id: "trace_12348"
  }
];

export const mockSessionAnalysis = {
  session: {
    id: 1,
    title: "Customer Support Bot Session",
    status: "completed",
    started_at: new Date(Date.now() - 1000 * 60 * 30).toISOString(),
    ended_at: new Date(Date.now() - 1000 * 60 * 5).toISOString(),
    total_latency_ms: 8500,
    total_tokens: 1200,
    total_cost_usd: 0.024
  },
  metrics: {
    total_spans: 4,
    successful_spans: 4,
    failed_spans: 0,
    success_rate: 100.0,
    avg_latency_ms: 2125.0
  },
  root_causes: [],
  bottlenecks: [
    {
      span_id: 1,
      span_name: "MainAgent",
      span_type: "agent",
      latency_ms: 5000,
      avg_latency_ms: 2125.0,
      latency_ratio: 2.35
    }
  ],
  token_analysis: {
    total_tokens: 1200,
    total_cost: 0.024,
    cost_per_token: 0.00002,
    llm_calls: 2,
    avg_tokens_per_call: 600
  },
  llm_traces: [
    {
      id: 1,
      model: "gpt-4",
      provider: "openai",
      latency_ms: 1250,
      tokens: 450,
      cost_usd: 0.0067,
      status: "success",
      created_at: new Date(Date.now() - 1000 * 60 * 5).toISOString()
    }
  ]
};

export const mockSessionsSummary = {
  total_sessions: 15,
  completed_sessions: 12,
  failed_sessions: 2,
  running_sessions: 1,
  success_rate: 80.0,
  avg_latency_ms: 9500,
  total_cost_usd: 0.45,
  total_tokens: 15600,
  time_range_hours: 24
};

export const mockAlerts = [
  {
    id: 1,
    severity: "CRITICAL",
    title: "High latency detected",
    description: "Average latency has exceeded threshold",
    metric: 8500.0,
    threshold: 5000.0,
    acknowledged: false,
    acknowledged_at: undefined,
    acknowledged_by: undefined,
    created_at: new Date(Date.now() - 1000 * 60 * 10).toISOString(),
    alert_type: "latency",
    metric_name: "avg_latency_ms",
    session_id: undefined,
    span_id: undefined,
    trace_id: undefined,
    metadata: { source: "monitoring", environment: "production", region: "us-east" },
    resolved_at: undefined
  },
  {
    id: 2,
    severity: "HIGH",
    title: "Error rate spike",
    description: "Error rate has increased significantly",
    metric: 8.5,
    threshold: 5.0,
    acknowledged: true,
    acknowledged_at: new Date(Date.now() - 1000 * 60 * 5).toISOString(),
    acknowledged_by: "user_1",
    created_at: new Date(Date.now() - 1000 * 60 * 15).toISOString(),
    alert_type: "error_rate",
    metric_name: "error_rate_pct",
    session_id: undefined,
    span_id: undefined,
    trace_id: undefined,
    metadata: { source: "automated_check", environment: "production", region: "us-west" },
    resolved_at: undefined
  },
  {
    id: 3,
    severity: "MEDIUM",
    title: "Cost threshold exceeded",
    description: "Daily cost limit has been reached",
    metric: 125.0,
    threshold: 100.0,
    acknowledged: false,
    acknowledged_at: undefined,
    acknowledged_by: undefined,
    created_at: new Date(Date.now() - 1000 * 60 * 20).toISOString(),
    alert_type: "cost",
    metric_name: "total_cost_usd",
    session_id: undefined,
    span_id: undefined,
    trace_id: undefined,
    metadata: { source: "monitoring", environment: "production", region: "eu-west" },
    resolved_at: undefined
  },
  {
    id: 4,
    severity: "LOW",
    title: "Request rate anomaly",
    description: "Unusual request pattern detected",
    metric: 150.0,
    threshold: 100.0,
    acknowledged: true,
    acknowledged_at: new Date(Date.now() - 1000 * 60 * 8).toISOString(),
    acknowledged_by: "user_2",
    created_at: new Date(Date.now() - 1000 * 60 * 25).toISOString(),
    alert_type: "token_usage",
    metric_name: "requests_per_minute",
    session_id: undefined,
    span_id: undefined,
    trace_id: undefined,
    metadata: { source: "user_report", environment: "staging", region: "asia-pacific" },
    resolved_at: new Date(Date.now() - 1000 * 60 * 2).toISOString()
  }
];

export const mockAlertsSummary = {
  by_severity: [
    { severity: "CRITICAL", total: 1, unacknowledged_count: 1 },
    { severity: "HIGH", total: 1, unacknowledged_count: 0 },
    { severity: "MEDIUM", total: 1, unacknowledged_count: 1 },
    { severity: "LOW", total: 1, unacknowledged_count: 0 }
  ],
  total_alerts: 4,
  unacknowledged_alerts: 2
};

export const mockAlertThresholds = [
  {
    id: 1,
    metric_name: "avg_latency_ms",
    threshold_value: 5000,
    severity: "HIGH",
    enabled: true,
    created_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    updated_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    description: "Average response time threshold"
  },
  {
    id: 2,
    metric_name: "error_rate_pct",
    threshold_value: 5.0,
    severity: "MEDIUM",
    enabled: true,
    created_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    updated_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    description: "Error rate percentage threshold"
  },
  {
    id: 3,
    metric_name: "total_cost_usd",
    threshold_value: 100.0,
    severity: "LOW",
    enabled: true,
    created_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    updated_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    description: "Daily cost threshold"
  },
  {
    id: 4,
    metric_name: "requests_per_minute",
    threshold_value: 100,
    severity: "MEDIUM",
    enabled: true,
    created_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    updated_at: new Date(Date.now() - 1000 * 60 * 60 * 24).toISOString(),
    description: "Request rate threshold"
  }
];
